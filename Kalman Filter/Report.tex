\documentclass[12pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsß with pdflatex; use eps in DVI mode
\usepackage{amsmath}
\usepackage{mathtools}
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\numberwithin{equation}{section}
\usepackage{color}



\title{Scripsie: State Estimation \& Observation}
\author{Aidan Landsberg}
\date{\today}							% Activate to display a given date or no date

\begin{document}
\maketitle
Dr. Van Daalen,\\\\
\textbf{This section, for the moment, provides initially, a general background to explain the content that follows. It will later be integrated into another, larger section in the final report, while other information will still be added to this particular section. The main goal is to set up the necessary theoretical basis in order to describe, amongst other thing, the motion model (prior to your approval). Also, I have purposefully omitted all relevant references as I am yet to completely study all the literature I have obtained. I am though, well aware of which sections of this write up is to be referenced and will proceed to do so at a later stage.}\\
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Single Camera Simultaneous Localisation and Mapping }
The ultimate goal of the approach presented here, is to obtain a probabilistic three dimensional (3D) map of features, representing at every time instance, the estimates of both the state of the camera as well as the positions of every feature observed. These features of interest are more commonly referred to as \textit{landmarks} and the aforementioned terms will, from hereon in, be used synonymously.  Most importantly though, the map is to contain the \textit{uncertainty} associated with each of the aforementioned estimates.\\The process regarding the construction of this map of features is to be implemented through the use of an (Extended) Kalman filter. The map initially, completely void of any landmarks, is recursively updated according to the subsequent fusions of both predictions and measurements presented to the Kalman filter. As new (potentially interesting) features are observed, the state estimates of both the camera as well as the landmarks are both updated - augmenting the state vector with additional features (if indeed they are observed) while deleting any landmarks that are of no longer of interest. In order to obtain the best possible result, the algorithm should strive to obtain a sparse set of higher-quality landmarks rather than a dense set of ordinary landmarks within the environment.    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    
\section{Kalman Filter}
The following section will provide a brief introduction to the \textit{Kalman Filter} (KF), the fundamental algorithm that enables the realisation of the SLAM problem. With regard to SLAM, a Kalman Filter can be briefly decried as an algorithm that \textbf{optimally} estimates the state of the robots pose as well as the position of the landmarks within the map, given process and measurement noise. The Extended Kalman filer, an extension of the general Kalman filter, aims to enable the modelling of non-linear systems. The fundamental principals regarding the Extended Kalman filter (EKF) are identical to those of the standard Kalman filter.\\
The Kalman filter is a popular, well studied technique for filtering and prediction of a \textit{continuous} linear system that contains uncertainty. It is realised utilising a recursive Gaussian filter (more specifically the Bayes filter) in order to estimate the state of the system accordingly, provided that the state vector $\textbf{\^{x}}_k$ is modelled by a single multivariate Gaussian distribution. %The system is to be observed at discrete steps in time - denoted by the subscript$k =1, 2,  3, ...$ - where at every individual time-step, it can be influenced by a set of actions. It is assumed that\\
\newpage   
The solution to this specific implementation of the Simultaneous Localisation and Mapping (SLAM) problem, takes the following probabilistic form:
\begin{equation}
P\big(\textbf{\^{x}}_k,\textbf{\^{y}}_k\hspace{0.15cm}|\hspace{0.15cm}\textbf{z}_{0:k},\textbf{u}_{0:k},\textbf{x}_0\big).
\end{equation}
 with the aforementioned distribution described at every time instance $k$. A brief description would yield that the distribution above, describes a joint density - at every given time instance $k$ - of the robot state as well as the landmark locations \textbf{given} all of the previously recorded observations and control inputs. It is assumed that a measurement function $h$ is used to obtain the measurements $\textbf{z}_{k}$ - these measurements also incorporate an uncertainty. Using the Kalman filter, each time-step $k$ can be separated into two (sequential) steps: the \textit{prediction step} and the \textit{update step}. Each of the aforementioned steps are discussed in more detail later in this section, but the general idea is that the Kalman filter firstly estimates the new state of the system according to the previous state as well as the control inputs, after which a measurement is predicted if indeed the system were to find itself within the estimated state. An actual measurement is then obtained through the system sensors to determine the actual state of the system. Ultimately, the actual state of the system is compared to the previously predicted state and the (optimally weighted) Kalman gain is adjusted accordingly to "correct" any errors incorporated from the previously mentioned steps. It is important to note that both the prediction as well as the measurement influence the state of the system.\\\\
The general implementation above, is only valid when considering linear systems. Considering that most practical systems of interest yield non-linear behaviour, the Kalman filter in its purest form is of little essence. Instead, the Kalman filter algorithm can be extended in order to accommodate non-linear behaviour. The linearisation process (to be explained later in this section) can briefly be described by using first order Taylor expansion to create a linear approximation of a non-linear function. The Taylor expansion creates an approximation that is linear and dependent on the properties of the functions derivative yielded from a single - generally that of the most likelihood - point (mean). \textit{Jacobians} are commonly used to linearise non-linear functions. It is therefore important - in order to successfully implement a Kalman filter on a non-linear system - to linearise both the state transition model as well as the measurement model; both of which are generally non-linear functions. Once achieved, the EKF; which behaves otherwise identically in terms of operation to the general Kalman filter, can be implemented upon non-linear systems.      
%This description then allows for the implementation of a recursive algorithm, namely, a discrete Kalman filter. In order for a Kalman filter to be successfully implemented, a \textbf{state transition (motion) model} as well as an \textbf{observation model} is required to individually describe the effects of the control input as well as the observations respectively.\\
%It is important to note that the Kalman filter estimates the state of a continuous- or discrete-time process that is described by a set of differential (continuous) or difference (discrete) equations. The Kalman filter then continuously updates the state estimates according to the measurements it obtains. This procedure, takes the form of a two-step recursive process: an a priori prediction (time-update) and an observation based correction (measurement-update). 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{State Representation}
Generally, a state can be defined as any facet that has the ability to impact the future. In the context of this particular paper, the states will comprise of all facets that impact the future of both the robot and the environment dynamics. As per the definition of the Kalman filter, it is essential that the system possesses a model to estimate future states. All relevant state estimates are embedded within the state vector $\textbf{\^{x}}_t$ which is comprised of two parts, the camera state $\textbf{\^{x}}_v$ and the feature estimates $\textbf{\^{y}}$ respectively. The camera state provides the estimate for the robot's pose at each time-step and the landmark estimates provide the landmark's estimated position within the map.\\
Mathematically, the probabilistic map can be represented through the state vector $\textbf{\^{x}}_t$ and a covariance matrix $\textbf{P}$. $\textbf{\^{x}}_t$, as previously mentioned, is a a single column vector containing the estimates of the camera as well as the landmark positions, and $\textbf{P}$ is a square matrix containing the covariances of each state with respect to every other state. These quantities can be mathematically shown as follows:
\begin{equation}
\textbf{\^{x}}_t = 
 \begin{pmatrix}
  \textbf{\^{x}}_v\\
  \textbf{\^{y}}_1 \\ 
  \textbf{\^{y}}_2 \\
  \vdots \\
  \textbf{\^{y}}_N
 \end{pmatrix} , \hspace{0.5cm}
P_{NN} =
 \begin{bmatrix}
  P_{x,x} & P_{x,{y_1}} & P_{x,{y_2}} & \cdots & P_{x,{y_N}} \\
  P_{{y_1},x} &  P_{{y_1},{y_1}} & P_{{y_1},{y_2}} & \cdots &  P_{{y_1},{y_N}} \\
  P_{{y_2},x} &  P_{{y_2},{y_1}} & P_{{y_2},{y_2}} & \cdots &  P_{{y_2},{y_N}} \\
  \vdots  & \vdots  & \vdots & \ddots & \vdots  \\
  P_{{y_N},x} & P_{{y_N},{y_1}} & P_{{y_N},{y_2}}& \cdots & P_{{y_N},{y_N}}
 \end{bmatrix}.
\end{equation}
\\These quantities then, allow us to approximate the uncertainty regarding the generated feature map as a $N$-dimensional single multi-variate Gaussian distribution, where $N$, as stated above, is the total number of state estimates within the state vector.\\
\\Before continuing, it is important to consider and understand the notation used in the sections that follow. Two separate coordinate systems are to be considered, namely the \textit{fixed} inertial reference frame system $W$ and the cameras free coordinate frame system, more commonly referred to as the body frame $C$. System variables defined within either of the aforementioned coordinate systems, are from here on in, to be designated a superscript to establish in which coordinate system it may be relevant (e.g. $x^{W}$). Derivatives of parameters are denoted through a dot symbol, second derivates are denoted through a double dot symbol and so forth; for instance the derivative of position $x$ will be denoted as $\dot{x}$ and its second derivative denoted as $\ddot{x}$. Vectors will be printed in bold and non-italics to better distinguish them from scalers. An example can be shown regarding the variable x: $\textbf{x}$ denotes a vector while its scalar counterpart would be represented as \textit{x}. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Camera Position State Representation}
The following concept describes a suitable method to represent all relevant information regarding the cameras position and orientation in a 3D space. According to most implementations of robot localisation, there exists no concern to contrast between the concepts of a camera state $\textbf{\^{x}}_v$ and a camera position state $\textbf{x}_p$: it is therefore important to note that a position state - containing the required information regarding a robots position - is merely an element of the camera state vector. The state camera vector - comprising of 13 individual states - is mathematically described as follows:
\begin{equation}
\textbf{\^{x}}_v=  
 \begin{pmatrix}
  \textbf{{r}}^W\\
  \textbf{{q}}^{WC} \\ 
  \textbf{{V}}^W\\
  \mathbf{\omega}^C
 \end{pmatrix} .
\end{equation}
where $\textbf{r}^W =$ (\textit{x} \textit{y} \textit{z}$)^T$ indicates the 3D cartesian position of the camera, $\textbf{{q}}^{WC}$ the unit orientation \textit{quarternion} - to be mathematically defined and described in the appendix - indicating the camera orientation (represented in the body frame) \textit{relative to the inertial reference frame} $W$, $\textbf{{V}}^W$ indicating the {linear} velocities of the camera relative to the inertial reference frame $W$ and $\mathbf{\omega}^C$ indicating the {angular} velocities of the camera relative to its own body frame $C$.     
Often, the modelling of dynamic systems require that additional parameters - separate to those describing the position and orientation of the robot - be included in the state vector along with the position state vector. This is illustrated in the description above, with the position state vector $\textbf{{x}}_p$ comprising of the 3D position vector, $\textbf{{r}}^W$ and the unit orientation \textit{quarternion}, $\textbf{{q}}^{WC}$. The linear and angular velocities, $\textbf{V}^W$ and $\mathbf{\omega}^C$, form the additional information required for system modelling. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   
\subsubsection{Cartesian Feature Representation}
As previously discussed, the aim is to describe a set of high-quality, well defined landmarks within the map. The map itself is to contain a 3D position of \textit{each} observed landmark  as well as a combined uncertainty. The feature estimates $\textbf{\^{y}}$ - comprising of $N$ landmarks - is mathematically described through three individual cartesian coordinates - $x$, $y$ and $z$ respectively:

\begin{equation}
\textbf{\^{y}}_n = (x_n\hspace{0.25cm}y_n\hspace{0.25cm}z_n)^T.
\end{equation}
where $n$ represents a specific single landmark. \\\\
With reference to the theory on image processing, it can be discussed that the depth of a given landmark (in this case the $z$-coordinate) cannot be immediately determined, but rather approximated via triangulation given the landmark is observed over a sequence of (minimally) two known camera positions. The $x$ and $y$ measurements however, can be immediately determined from the image plane.  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Control Inputs}
%\textbf{Still reading up on some literature before properly defining this section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsubsection{Inverse Depth Feature Representation}   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Prediction Step}
With reference to the probabilistic form of the solution to the SLAM problem, the prediction step requires a description in terms of a probability distribution. The description of the aforementioned motion model can then, in terms of the probability distribution on the state transitions, take the following form:
\begin{equation}
\begin{split}
P(&\textbf{\^{x}}_k\hspace{0.15cm}|\hspace{0.15cm}\textbf{\^{x}}_{k-1}, \textbf{{u}}_k) \\
&=\cfrac{1}{\sqrt{|2\pi\textbf{R}_w|}}\hspace{0.1cm}exp\hspace{0.1cm}\Big(\frac{1}{2}(\textbf{\^{x}}_k-\textbf{\^{x}}_{k\to k+1})^T\textbf{R}^{-1}_w(\textbf{\^{x}}_k-\textbf{\^{x}}_{k\to k+1})\Big).
\end{split}
\end{equation} 
where $\textbf{\^{x}}_{k\to k+1}$ represents the motion model. \\\\
The motion model is assumed to take the form of a Markov process, yielding that the next state $\textbf{\^{x}}_k$ is only dependent upon the state immediately preceding it - $\textbf{\^{x}}_{k-1}$ - as well as the input control $\textbf{{u}}_k$. Additionally, it is important to note that the uncertainty regarding the motion model is independent of the uncertainty regarding both the observation model as well as that of the probabilistic map itself.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{State Space Model} 
As previously discussed, the Kalman Filter requires a state transition (motion) model in order to estimate the current state of the system. In short, the motion model describes the transition from the previous state to the following state with regard to the robots kinematic motion as well as the control inputs. The motion model in this particular instance can be described through a \textbf{linear} differential equation of the following form:
\begin{equation}
\textbf{\.{x}}(t) = \textbf{A}\textbf{x}(t) + \textbf{B}\textbf{u}(t)+\textbf{w}(t).
\end{equation} 
where the state matrix $\textbf{A}$, describes the manner in which state evolves from time $t-1$ to $t$ without the influence of noise and controls, the input matrix $\textbf{B}$, describes how the control vector $\textbf{u}(t)$ evolves from time step $t-1$ to $t$ and $\textbf{w}(t)$ is a \textbf{zero-mean} Gaussian random variable representing the process noise with a covariance matrix $\textbf{R}_w$.\\\\
Considering that the Kalman Filter is a recursive, numerical evaluation, it is necessary to convert the previously defined continuous model into its discrete counterpart. Various methods of discretisation exist, though this specific implementation makes use of the forward difference/Euler’s method. This method  \textit{approximates} the derivative for a state for a sampling period $\Delta T$ as follows:  
\begin{equation}
\begin{split}
\textbf{\.{x}}[k] &= \lim_{\Delta T\to 0}{\frac{\textbf{x}[k+1]-\textbf{x}[k]}{\Delta T}} 		 \\										\Delta T\textbf{\.{x}}[k] &\approx \textbf{x}[k+1]-\textbf{x}[k]. \\
\end{split}
\end{equation}     
The state estimate of the discrete counterpart at the following sampling instance, namely $k + 1$, is then presented as follows (given a small enough sampling instance $\Delta T$):
\begin{equation}
\begin{split}
\textbf{x}[k+1] &= \big(\textbf{I}+\textbf{A}\Delta T\big)\textbf{x}[k] + \textbf{B}\textbf{u}[k]\Delta T + \textbf{w}[k]\Delta T.
\end{split}
\end{equation}
where $\big(\textbf{I}+\textbf{A}\Delta T\big) = \textbf{A}_d$ is the discrete state matrix, $ \textbf{B}\Delta T = \textbf{B}_d$ is the discrete input matrix and $\textbf{w}[k]\Delta T=\textbf{w}_d[k]$ is the discrete input process noise. \\\\
Ultimately, the form of the final difference equation describing the system at each individual sampling instance is given as follows:
\begin{equation}
\textbf{x}[k+1]= \textbf{A}_d\textbf{x}[k] + \textbf{B}_d\textbf{u}[k]+\textbf{w}_d[k].
\end{equation} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{State Transition: Linear Model}
In order to derive the motion model for the system at hand, it is vital that the certain characteristics of the system be understood. Firstly, the robot system - from here on in to be referred to as the \textbf{camera} - is comprised of a monocular camera and an attached Inertial Measurement Unit (IMU) package. Secondly, the camera is to be considered as a six degree of freedom (DOF) rigid body. Briefly the six DOF describe the camera's three \textit{translational} and three \textit{rotational} degrees of freedom. \\
We therefore set out to define a kinematic motion model - using Newton's laws of motion - to describe the cameras movement through the environment as a result of initially unknown, external inputs to the system. Lastly, it should be stressed that embedded within the motion model, should be the impacts of uncertainty through both internal and external factors. 
%It is assumed in this instance, that at each time-step, an unknown angular acceleration $\mathbf{\Omega}^R$ acts upon the system. This input is modelled as a zero-mean Gaussian process that causes an impulse of angular velocity:
%\begin{equation}
%\textbf{w}_d[k]  = \textbf{w}[k] \Delta T =     
% \begin{bmatrix}
% \mathbf{\Omega}^R
% \end{bmatrix} = 
%  \begin{pmatrix}
%  	\alpha_x \Delta T \\
% 	\alpha_y \Delta T \\
%\alpha_z \Delta T \\
% \end{pmatrix} .
%\end{equation}  
%with a covariance matrix $\textbf{R}_w$ that is assumed as a diagonal initially, to represent uncorrelated noise in all of the rotational components.\\
%With reference to the previously defined state motion model in (1.8)
It must also be stressed that initially, a stochastic, linear discrete-time model is adopted to approximate the motion model. Using the kinematic equations of linear and angular motion, it is aimed to ultimately and complete the previously defined state space model. We begin by describing all relevant states and inputs:
\begin{equation}
\begin{split}
\textbf{x}[k] &= \big[\textit{x}_{k}\hspace{0.25cm}\textit{y}_{k}\hspace{0.25cm}\textit{z}_{k}\hspace{0.25cm}\textit{\.{x}}_{k}\hspace{0.25cm}\textit{\.{y}}_{k}\hspace{0.25cm}\textit{\.{z}}_{k}\hspace{0.05cm}\hspace{0.25cm}\textit{q}_{0,k}\hspace{0.25cm}\textit{q}_{1,k}\hspace{0.25cm}\textit{q}_{2,k}\hspace{0.25cm}\textit{q}_{3,k}\hspace{0.25cm}{\omega}_{x,k}\hspace{0.25cm}{\omega}_{y,k}\hspace{0.25cm}{\omega}_{z,k}\hspace{0.1cm}\big]^T \\
\textbf{u}[k] &= \big[\hspace{0.1cm}\ddot{x}_{k}\hspace{0.25cm}\ddot{y}_{k}\hspace{0.25cm}\ddot{z}_{k}\hspace{0.25cm}\textit{\.{q}}_{0,k}\hspace{0.25cm}\textit{\.{q}}_{1,k}\hspace{0.25cm}\textit{\.{q}}_{2,k}\hspace{0.25cm}\textit{\.{q}}_{3,k}\hspace{0.1cm}\big]^T\\
\end{split}
\end{equation}
and extend the discrete-time difference equation describing the system to incorporate the motion model,  
\begin{equation}
\begin{split}
\textbf{x}[k+1] &= \textbf{A}_d\textbf{x}[k] + \textbf{B}_d\textbf{u}[k]+\textbf{w}_d[k] \\
\textbf{A}_d&= 
 \begin{bmatrix}
  1 & 0 & 0 & \Delta T & 0 & 0 & 0 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 & \Delta T & 0 & 0 & 0 & 0 & 0 \\
  0 & 0 & 1 & 0 & 0 & \Delta T & 0 & 0 & 0 & 0 \\
  0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
  0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
  0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
  0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
  0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
 \end{bmatrix} = (\textbf{I}+\textbf{A}\Delta T\big)  \\
 \textbf{B}_d&=
\begin{bmatrix}
  \Delta T & 0 & 0 & 0 & 0 & 0 & 0 \\
  0 & \Delta T & 0 & 0 & 0 & 0 & 0 \\
  0 & 0 & \Delta T & 0 & 0 & 0 & 0 \\
  0 & 0 & 0 & \Delta T & 0 & 0 & 0 \\
  0 & 0 & 0 & 0 & \Delta T & 0 & 0 \\
  0 & 0 & 0 & 0 & 0 & \Delta T & 0 \\
  0 & 0 & 0 & 0 & 0 & 0 & \Delta T \\
\end{bmatrix} = \textbf{B}\Delta T \\\\
 \textbf{w}_d[k] &=\mathcal{N}(0,  \textbf{R}_w) =  
 \begin{bmatrix}
 \mathbf{\Omega}^R
 \end{bmatrix} = \textbf{w}[k] \Delta T 
 \end{split}
\end{equation}
it can be observed from the model above that the motion model adheres to the forward method of discretisation derived in (2.8). The motion model also adheres to the Markov process assumption, in that it can be completely described through only its transition from the previous state as well as the control inputs.   
\subsubsection{Non-Linear Modelling} 
\subsubsection{State Transition: Non-Linear Model} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Correction Step}
With reference again to the probabilistic form of the solution to the SLAM problem, the measurement step too, requires a description in terms of a probability distribution. The observation model however, models the uncertainty regarding a measurement taken at an instance $\textbf{\^{z}}_k$ given that the locations of both the robot as well as the landmarks are known. This uncertainty can be described in the following form:  
\begin{equation}
\begin{split}
P(&\textbf{\^{z}}_k\hspace{0.1cm}|\hspace{0.15cm}\textbf{\^{x}}_{k}, \textbf{\^{y}}_k) \\
&=\cfrac{1}{\sqrt{|2\pi\textbf{Q}_v|}}\hspace{0.1cm}exp\hspace{0.1cm}\Big(\frac{1}{2}(\textbf{\^{z}}_k-\textbf{\^{z}}_{k\to k+1})^T\textbf{Q}^{-1}_v(\textbf{\^{z}}_k-\textbf{\^{z}}_{k\to k+1})\Big).
\end{split}
\end{equation} 
where $\textbf{\^{z}}_{k\to k+1}$ represents the measurement model. \\\\
It can be (reasonably) assumed that the uncertainty regarding the measurements are conditionally independent given the uncertainty regarding the robot and landmark locations if indeed they are completely defined. Also, the correction step seeks to obtain the difference between the actual measurements $\textbf{\^{z}}_k$ and the predicted measurements. These predicted measurements are to be obtained through an observation model that we from hereon in refer to as the measurement function, denoted as $\textbf{h}_i$.  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Measurement Function}
The correction step of the Kalman filter aims to ultimately correct the previously estimated robot pose and landmark position through exterior sensor measurements. The measurement process generally involves a measurement estimate that incorporates an uncertainty. With regard to the implantation proposed in this paper, landmarks are required to be observed and measured through the use of a camera. To mathematically describe this process, the previously mentioned measurement function is used to effectively model the measurement estimation. It is essential that the measurement function, like the motion model, be \textbf{linear} in nature and additionally, the measurement function {must} describe the position of a \textbf{point} feature with regard to the previously estimated states - namely the robot pose and the landmark positions. \\\\   
Considering that the camera observations are obtained with regard to its own reference frame $C$, the definition of the measurement function is adapted in order to be described with regard to the inertial reference frame. The measurement function $\textbf{h}^W_i$ that describes a directional vector in relation to the cameras body frame is thus mathematically defined as follows: 
\begin{equation}
\textbf{h}^W_i = \textbf{R}^{CW}\big(\textbf{y}^W_i-\textbf{r}^W\big) = 
  \begin{pmatrix}
  \begin{pmatrix}
  x_i\\
  y_i \\ 
  z_i \\
  \end{pmatrix} - \textbf{r}^{W} 
  \end{pmatrix}  
\end{equation}
where the subscript $i$ corresponds a directional vector $\textbf{h}^C$ to its cartesian point $\textbf{y}^W$, $\textbf{r}^W$ describes the cartesian position of the camera, $\textbf{y}^W$ describes the cartesian position of a given landmark and $\textbf{R}^{CW}$ represents the rotational matrix that is required to transform the aforementioned positional vectors from the inertial reference frame into the cameras body frame coordinate system.\\\\
With reference to Chapter 2 regarding perspective cameras, it can be recalled that a given features position is described by a 2-dimensional position of the image frame of the camera. Recalling, the standard pinhole camera model defines this position mathematically as follows:
 \begin{equation}
\textbf{h}_i = 
  \begin{pmatrix}
  u_i\\
  v_i \\ 
  \end{pmatrix} =
    \begin{pmatrix}
  u_0 - fk_u\frac{h^R_{i,x}}{h^R_{i,z}}\\
  v_0 - fk_v\frac{h^R_{i,y}}{h^R_{i,z}} \\ 
  \end{pmatrix} 
\end{equation}
where $fk_u$, $fk_v$, $u_0$ and $v_0$ are the previously described camera calibration parameters.\\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Feature Tracking}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{System Update}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 














\end{document}   

  %\begin{bmatrix}
  %x_k\\
  %y_k \\ 
  %z_k \\
  %q_{0,k}\\
  %q_{1,k}\\
  %q_{2,k}\\
  %q_{3,k}\\
  %\dot{x}_k\\
  %\dot{y}_k\\
  %\dot{z}_k\\
  %\end{bmatrix}  
  %The position state $\textbf{{x}}_p$ can furthermore be fully represented as follows:
%\begin{equation}
%\textbf{{x}}_p=  
% \begin{pmatrix}
% x\\
% y\\ 
% z\\
% q_0\\
% q_1\\
% q_2\\
% q_3\\
% \end{pmatrix} .
%\end{equation}  
%Various alternative implementations exist to represent a robots pose in a 3D space, each presenting their own unique advantages (and disadvantages) with respect to the others. A representation of an arbitrary 3D position and orientation, requires at least, three parameters describing the cartesian position as well as an additional three describing the orientation. This specific implementation, utilises the \textbf{quarternion} representation to portray the orientation information and thus requires an additional parameter to aid its description. 